{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## In this notebook we try classification with two techniques :\n",
        "\n",
        "- Zero shot classification\n",
        "- Sentence similarity"
      ],
      "metadata": {
        "id": "RgF0Cr_qq1Za"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Eventual installations\n",
        "!pip install transformers\n",
        "!pip install -U sentence-transformers"
      ],
      "metadata": {
        "id": "6NCHznc4p1rq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "from transformers import pipeline\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "from sklearn.metrics import multilabel_confusion_matrix\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time"
      ],
      "metadata": {
        "id": "q05PSskq2um5"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 0. Loading data and tranform it into vectors/tensors"
      ],
      "metadata": {
        "id": "9l-Ca22Pq1NG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "PATH = ''\n",
        "df = pd.read_csv(PATH+'intent-detection-train.csv')\n",
        "X = df['text'].tolist()\n",
        "Y = df['label'].tolist()\n",
        "Y_splitted = df['label'].apply(lambda x: x.replace('_', ' ')).tolist()\n",
        "labels = list(df['label'].apply(lambda x: x.replace('_', ' ')).unique())"
      ],
      "metadata": {
        "id": "1uFeg9gf53cB"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 1. Try zero-shot classification"
      ],
      "metadata": {
        "id": "zYZQ0YyHrDV0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "classifier = pipeline(\"zero-shot-classification\", model=\"MoritzLaurer/mDeBERTa-v3-base-mnli-xnli\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "52qegLm3DbJs",
        "outputId": "f98fb283-d1a1-4661-dbd2-760eadb55d26"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/convert_slow_tokenizer.py:473: UserWarning: The sentencepiece tokenizer that you are converting to a fast tokenizer uses the byte fallback option which is not implemented in the fast tokenizers. In practice this means that the fast version of the tokenizer can produce unknown tokens whereas the sentencepiece version would have converted these unknown tokens into a sequence of byte tokens matching the original piece of text.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# classifier = pipeline(\"zero-shot-classification\",\n",
        "#                        model=\"vicgalle/xlm-roberta-large-xnli-anli\")"
      ],
      "metadata": {
        "id": "S-bEY5FS2utH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "classifier_distilC = pipeline(\n",
        "    task='zero-shot-classification',\n",
        "    model=\"cmarkea/distilcamembert-base-nli\",\n",
        "    tokenizer=\"cmarkea/distilcamembert-base-nli\"\n",
        ")"
      ],
      "metadata": {
        "id": "0PYs1sjIEfHJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Labels in french\n",
        "labels_french = ['traduction', 'alerte de voyage', 'statut de vol',\n",
        "          'perte de baggage', 'recommandation de voyage',\n",
        "          'informations sur les bagages à main', 'réserver un hôtel',\n",
        "          'réserver un vol', 'autre',\n",
        "          ]"
      ],
      "metadata": {
        "id": "L0PjQTgz0xHT"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mapping between french and english labels\n",
        "# It will be usefull later\n",
        "french_to_english_label_map = {f: e for f,e in zip(labels_french, labels)}"
      ],
      "metadata": {
        "id": "OhlGnpeQrr4k"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We test MoritzLaurer/mDeBERTa-v3-base-mnli-xnli model from hugging face\n",
        "results = []\n",
        "for x,y in tqdm(zip(X,Y)):\n",
        "    results.append(classifier(x, labels))\n",
        "    print(results[-1]['labels'][0], ' , real = ', y)"
      ],
      "metadata": {
        "id": "LSYV_et16JZE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We test cmarkea/distilcamembert-base-nli model from hugging face\n",
        "\n",
        "# With english labels\n",
        "results = []\n",
        "for x in tqdm(X):\n",
        "    results.append(classifier_distilC(sequences = x, candidate_labels=labels))\n",
        "    #print(results[-1]['labels'][0], ' , real = ', y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KFHcFkNnEwdy",
        "outputId": "7207dd3e-7441-45a2-9778-51883006d4af"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 75/75 [00:46<00:00,  1.62it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We get the labels predicted\n",
        "results_labels = np.array([r['labels'][0] for r in results])\n",
        "\n",
        "# We get the accuracy\n",
        "accuracy = (np.array(results_labels)==np.array(Y_splitted)).sum()/len(results_labels)*100\n",
        "\n",
        "# Print results\n",
        "print('Accuracy is : ', round(accuracy, 2), '%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XBiOI2LgpDeG",
        "outputId": "f6ae0a5f-146e-43f3-d026-c783f6e29ab3"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy is :  13.33 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# With french labels\n",
        "results = []\n",
        "for x in tqdm(X):\n",
        "    results.append(classifier_distilC(sequences = x, candidate_labels=labels_french,))\n",
        "    #print(results[-1]['labels'][0], ' , real = ', y)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_XgE75JvreB_",
        "outputId": "008f49f3-59ed-49f0-ba9d-db63327112bb"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 75/75 [00:47<00:00,  1.59it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# We get the labels predicted\n",
        "results_labels = np.array([french_to_english_label_map[r['labels'][0]] for r in results])\n",
        "\n",
        "# We get the accuracy\n",
        "accuracy = (np.array(results_labels)==np.array(Y_splitted)).sum()/len(results_labels)*100\n",
        "\n",
        "# Print results\n",
        "print('Accuracy is : ', round(accuracy, 2), '%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mMND0wdvrnB6",
        "outputId": "1938984e-835f-4983-d235-eb25e10c2c2a"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy is :  17.33 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that this classification method don't offer good results (random guess is 11.11%). Let's try another one."
      ],
      "metadata": {
        "id": "6DBKb__9KPFO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2. Sentence similarity (Our best method found yet)\n",
        "\n"
      ],
      "metadata": {
        "id": "LWU284aVrJIo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the model\n",
        "model = SentenceTransformer('sentence-transformers/LaBSE')"
      ],
      "metadata": {
        "id": "n1XYQ0qG-kr7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#model.save('model_saved')"
      ],
      "metadata": {
        "id": "1vkYCAYcMfcZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#model_bis = SentenceTransformer('model_saved')"
      ],
      "metadata": {
        "id": "uJHZ2Zd1Mxr1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Start counting to get check if the model is fast\n",
        "start_s = time.time()\n",
        "\n",
        "# Get the embeddings\n",
        "X_embedded = model.encode(X)"
      ],
      "metadata": {
        "id": "MGA3xpvCcEVw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compte results\n",
        "# Here -np.inf for i==j since it is the same sentence\n",
        "results = [\n",
        "    [util.pytorch_cos_sim(x1, x2).numpy()[0,0] if i!=j else -np.inf for j, x2 in enumerate(X_embedded)]\n",
        "    for i, x1 in enumerate(X_embedded)\n",
        "    ]"
      ],
      "metadata": {
        "id": "OK_UgrlwuFVZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We get the index of the most similar sentence from our train dataset, for each input\n",
        "indexes = [np.argmax(r) for r in results]\n",
        "\n",
        "# We then get the corresponding line in the dataframe\n",
        "res = df.loc[indexes]"
      ],
      "metadata": {
        "id": "lckY0R-R_5TH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We get the accuracy\n",
        "accuracy = (res['label'].to_numpy()==df['label'].to_numpy()).sum()/len(df)*100\n",
        "\n",
        "# We print accuracy\n",
        "end_s = time.time()\n",
        "print('Accuracy : ', round(accuracy, 2), ' %')\n",
        "print('Computed ', len(X), ' results in ', round(end_s-start_s, 2), 's ('+str(round((end_s-start_s)/len(X), 4))+'s/sentence)')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p_cfOEwDBR1d",
        "outputId": "3da344a2-ba30-4947-9a4f-461fa679450e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy :  89.33  %\n",
            "computed  75  results in  12.85 s (0.1714s/sentence)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Here we plot the confusion matrix wrt lost luggages label\n",
        "conf_matrix = multilabel_confusion_matrix(y_true = df['label'].to_numpy(), y_pred = res['label'].to_numpy(), labels = ['lost_luggage'])[0]\n",
        "print('Confusion matrix for lost luggage label :')\n",
        "print(conf_matrix)\n",
        "print('Percentage of Sentences classified as \"lost luggage\" intent which weren\\'t \\n (false positives since here lost luggage is labelled as 0): ', round(conf_matrix[1,0]/conf_matrix.sum(), 2), '%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P2_m2QvtDpuE",
        "outputId": "5b4e63a6-7ba6-433a-93e0-7debcb60a86e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confusion matrix for lost luggage label :\n",
            "[[68  0]\n",
            " [ 0  7]]\n",
            "Percentage of Sentences classified as \"lost luggage\" intent which weren't \n",
            " (false positives since here lost luggage is labelled as 0):  0.0 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Limits of this method :\n",
        "- \"out of scope\" are luckily well classified for this train dataset but could perform bad on further test examples."
      ],
      "metadata": {
        "id": "6OBDU9bTzSJt"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oSV2xfdV7LgK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cjbRBqkDC86b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eDO5dcmwC9BK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Annex : We test our function on a \"test set\" like dataset"
      ],
      "metadata": {
        "id": "yQZF2EXHC2h4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def test():\n",
        "    s_start = time.time()\n",
        "\n",
        "    df_test = pd.read_csv(PATH+'intent-detection-train.csv')\n",
        "    X_test = df_test['text'].tolist()\n",
        "\n",
        "    # Embed the input to the vectorized representation\n",
        "    X_embedded_test = model.encode(X_test)\n",
        "\n",
        "    # Now we compute the cosine similarity with the training dataset\n",
        "    results = [\n",
        "        [util.pytorch_cos_sim(x1, x2).numpy()[0,0] for j, x2 in enumerate(X_embedded)]\n",
        "        for i, x1 in enumerate(X_embedded_test)\n",
        "        ]\n",
        "\n",
        "    # We get the index of the most similar sentence from our train dataset, for each input\n",
        "    indexes = [np.argmax(r) for r in results]\n",
        "\n",
        "    # We then get the corresponding line in the dataframe\n",
        "    predicted_similar_sentences = df.loc[indexes]\n",
        "\n",
        "    # We compute the accuracy of our model on the test set\n",
        "    accuracy = (predicted_similar_sentences['label'].to_numpy()==df_test['label'].to_numpy()).sum()/len(df_test)*100\n",
        "\n",
        "    conf_matrix = multilabel_confusion_matrix(\n",
        "        y_true = df_test['label'].to_numpy(),\n",
        "        y_pred = predicted_similar_sentences['label'].to_numpy(),\n",
        "        labels = ['lost_luggage'],\n",
        "        )[0]\n",
        "\n",
        "\n",
        "    # Print the results of our models\n",
        "    s_end = time.time()\n",
        "\n",
        "    # Print the results of our models\n",
        "    # Accuracy\n",
        "    print('Accuracy for this test set is :', round(accuracy, 2), '%')\n",
        "    # False negative are the special case we want to avoid\n",
        "    print('Percentage of sentences classified as \"lost luggage\" which weren\\'t :', round(conf_matrix[1,0]/conf_matrix.sum(), 2) , '%')\n",
        "    # Show rapidity of our model\n",
        "    print('computed ', len(X_test), ' results in ', round(s_end-s_start, 2), 's ('+str(round((s_end-s_start)/len(X), 4))+'s/sentence)')\n",
        "\n"
      ],
      "metadata": {
        "id": "jDgNOdVJ_6SO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We test this function\n",
        "# the 100% accuracy is expected as sentences are compared to themselves. And hence are similar.\n",
        "test()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xsi272ptAN1d",
        "outputId": "cc75f319-6f97-4713-ebe3-b79855459aa2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy for this test set is : 100.0 %\n",
            "Percent of sentences classified as \"lost luggage\" which weren't : 0.0 %\n",
            "computed  75  results in  8.14 s (0.1085s/sentence)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "A1F9Nx-VARav"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}